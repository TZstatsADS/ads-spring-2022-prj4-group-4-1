{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries ...\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGORITHM 1: LOCAL MASSAGING\n",
    "# inputs - dataframe, sensitive attribute (\"race\" in this case), explanatory attribute (v_decile_score), label attribute (two_year_recid)\n",
    "# outputs - output_dictionary (a dictionary of dataframes subgrouped oby explanatory attribute )\n",
    "\n",
    "def local_massage(dataframe, s_attribute, e_attribute, y_attribute, param): \n",
    "    \n",
    "    # creates sub dictionaries for each category of score\n",
    "    dictionary = partition(dataframe, e_attribute)\n",
    "    \n",
    "    # dictionary to store our output\n",
    "    output_dictionary = {}\n",
    "    \n",
    "    # for each category of score\n",
    "    for key in dictionary.keys(): \n",
    "        tmp_df = dictionary[key]\n",
    "        \n",
    "        # learn a ranker H (logistic regression, which will give us probabilities of y = 0 or 1)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(tmp_df.drop(columns=['two_year_recid'], axis=1), tmp_df.two_year_recid.copy())\n",
    "            \n",
    "        # add these predictions to the dataframe\n",
    "        tmp_df['prob_0'], tmp_df['prob_1'] = model.predict_proba(tmp_df.drop(columns=['two_year_recid'], axis=1)).T\n",
    "        \n",
    "        # find the number of caucassians to switch using algorithm 4\n",
    "        no_of_caucasian_switch = int(subroutine(dataframe, s_attribute, 1, e_attribute, key, y_attribute)*param)\n",
    "        no_of_aa_switch = int(subroutine(dataframe, s_attribute, 0, e_attribute, key, y_attribute)*param)\n",
    "\n",
    "        # create subtables (these encompass the data)\n",
    "        # we use these to make finding the top X closest to the boundary easier\n",
    "        caucasian_df_pos = tmp_df[(tmp_df['race'] == 1) & (tmp_df['two_year_recid'] == 1)]\n",
    "        caucasian_df_neg = tmp_df[(tmp_df['race'] == 1) & (tmp_df['two_year_recid'] == 0)] \n",
    "        aa_df_pos = tmp_df[(tmp_df['race'] == 0) & (tmp_df['two_year_recid'] == 1)]\n",
    "        aa_df_neg = tmp_df[(tmp_df['race'] == 0) & (tmp_df['two_year_recid'] == 0)]\n",
    "        \n",
    "        # sort to make finding the closest to the boundary easier\n",
    "        # here, we are only concerned with switch 0 caucasians to 1 and 1 african-americans to 0\n",
    "        caucasian_df_neg['dist'] = abs(caucasian_df_neg['prob_0'] - 0.5)\n",
    "        caucasian_df_neg = caucasian_df_neg.sort_values(by='dist')\n",
    "        aa_df_pos['dist'] = abs(aa_df_pos['prob_0'] - 0.5)\n",
    "        aa_df_pos = aa_df_pos.sort_values(by='dist')\n",
    "        \n",
    "        caucasian_df_neg = caucasian_df_neg.reset_index(drop=True)\n",
    "        aa_df_pos = aa_df_pos.reset_index(drop=True)\n",
    "        \n",
    "        # relabel the closest \"no_of_caucasian_switch\" of the caucasians\n",
    "        for i in range(no_of_caucasian_switch): \n",
    "            caucasian_df_neg.at[i, 'two_year_recid'] = 1\n",
    "            \n",
    "        # relabel the closest \"no_of_AA_switch\" of the african-americans\n",
    "        for i in range(no_of_aa_switch): \n",
    "            aa_df_pos.at[i, 'two_year_recid'] = 0\n",
    "\n",
    "        # join all the 4 subsets (now with new labels) and store in output_dictionary\n",
    "        output_dictionary[key] = pd.concat([caucasian_df_pos, caucasian_df_neg, aa_df_pos, aa_df_neg], ignore_index=True)\n",
    "    \n",
    "    # return the new dataframe with the labels\n",
    "    return output_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGORITHM 2: LOCAL PREFERENTIAL SAMPLING\n",
    "# inputs - dataframe, sensitive attribute (\"race\" in this case), explanatory attribute (v_decile_score), label attribute (two_year_recid)\n",
    "# outputs - output_dictionary (a dictionary of dataframes subgrouped oby explanatory attribute )\n",
    "\n",
    "def local_preferential_sampling(dataframe, s_attribute, e_attribute, y_attribute, param):\n",
    "    \n",
    "    # creates sub dictionaries for each category of score\n",
    "    dictionary = partition(dataframe, e_attribute)\n",
    "    \n",
    "    # dictionary to store our output\n",
    "    output_dictionary = {}\n",
    "    \n",
    "    # for each category of score\n",
    "    for key in dictionary.keys(): \n",
    "        tmp_df = dictionary[key]\n",
    "\n",
    "    \n",
    "        # learn a ranker H (logistic regression, which will give us probabilities of y = 0 or 1)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(tmp_df.drop(['two_year_recid'], axis=1), tmp_df.two_year_recid.copy())\n",
    "        \n",
    "        # add these predictions to the dataframe\n",
    "        tmp_df['prob_0'], tmp_df['prob_1'] = model.predict_proba(tmp_df.drop(['two_year_recid'], axis=1)).T\n",
    "        \n",
    "        # find the number of caucassians to delete/duplicate using algorithm 4\n",
    "        no_of_caucasian_switch = int((subroutine(dataframe, s_attribute, 1, e_attribute, key, y_attribute)/2)*param)\n",
    "        no_of_aa_switch = int((subroutine(dataframe, s_attribute, 0, e_attribute, key, y_attribute)/2)*param)\n",
    "        \n",
    "        # create subtables (these encompass the data)\n",
    "        # we use these to make finding the top X closest to the boundary easier\n",
    "        caucasian_df_pos = tmp_df[(tmp_df['race'] == 1) & (tmp_df['two_year_recid'] == 1)]\n",
    "        caucasian_df_neg = tmp_df[(tmp_df['race'] == 1) & (tmp_df['two_year_recid'] == 0)] \n",
    "        aa_df_pos = tmp_df[(tmp_df['race'] == 0) & (tmp_df['two_year_recid'] == 1)]\n",
    "        aa_df_neg = tmp_df[(tmp_df['race'] == 0) & (tmp_df['two_year_recid'] == 0)]\n",
    "        \n",
    "        caucasian_df_neg['dist'] = abs(caucasian_df_neg['prob_0'] - 0.5)\n",
    "        caucasian_df_pos['dist'] = abs(caucasian_df_pos['prob_0'] - 0.5)\n",
    "        aa_df_pos['dist'] = abs(aa_df_pos['prob_0'] - 0.5)\n",
    "        aa_df_neg['dist'] = abs(aa_df_neg['prob_0'] - 0.5)\n",
    "        \n",
    "        # sort to make finding the closest to the boundary easier\n",
    "        caucasian_df_neg = caucasian_df_neg.sort_values(by='dist')\n",
    "        caucasian_df_pos = caucasian_df_pos.sort_values(by='dist')\n",
    "        aa_df_neg = aa_df_neg.sort_values(by='dist')\n",
    "        aa_df_pos = aa_df_pos.sort_values(by='dist')\n",
    "        \n",
    "        caucasian_df_neg = caucasian_df_neg.reset_index(drop=True)\n",
    "        caucasian_df_pos = caucasian_df_pos.reset_index(drop=True)\n",
    "        aa_df_neg = aa_df_neg.reset_index(drop=True)\n",
    "        aa_df_pos = aa_df_pos.reset_index(drop=True)\n",
    "                \n",
    "        # delete the closest negative caucasians\n",
    "        for i in range(no_of_caucasian_switch): \n",
    "            caucasian_df_neg = caucasian_df_neg.drop([caucasian_df_neg.index[0]])\n",
    "            \n",
    "        # delete the closest positive aas\n",
    "        for i in range(no_of_aa_switch): \n",
    "            aa_df_pos = aa_df_pos.drop([aa_df_pos.index[0]])\n",
    "            \n",
    "        # duplicate the closest positive caucasians\n",
    "        caucasian_df_pos = pd.concat([caucasian_df_pos, caucasian_df_pos[:no_of_caucasian_switch]])\n",
    "            \n",
    "        # duplicate the closest negative aas\n",
    "        aa_df_neg = pd.concat([aa_df_neg, aa_df_neg[:no_of_aa_switch]])\n",
    "            \n",
    "        # join all the 4 subsets (now with new labels) and store in output_dictionary\n",
    "        output_dictionary[key] = pd.concat([caucasian_df_pos, caucasian_df_neg, aa_df_pos, aa_df_neg], ignore_index=True)\n",
    "    \n",
    "    return output_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGORITHM 3: PARTITION\n",
    "# input - dataframe (pandas dataframe), e_attribute (string, explanatory attribute)\n",
    "# output - a dictionary of dataframes where key is a unique e_attribute, and the dataframes\n",
    "# contain all rows from the original dataframe where e_attribute is the key\n",
    "\n",
    "def partition(dataframe, e_attribute): \n",
    "    \n",
    "    dataframe_dict = {}\n",
    "    \n",
    "    unique_e = dataframe[e_attribute].unique()\n",
    "    \n",
    "    for e in unique_e: \n",
    "        \n",
    "        tmp_df = dataframe[(dataframe[e_attribute]) == e]\n",
    "        dataframe_dict[e] = tmp_df\n",
    "    \n",
    "    return dataframe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGORITHM 4: SUBROUTINE DELTA(s_attribute)\n",
    "# input - dataframe (entire pandas dataframe), s_attribute (name of s_attribute), s_attribute_value (value of s_attribute)\n",
    "# e_attribute (name of e_attribute), e_attribute_value (value of e_attribute), y_attribute (name of y_attribute)\n",
    "# output - an integer indicating the number of people to flip labels\n",
    "\n",
    "def subroutine(dataframe, s_attribute, s_attribute_value, e_attribute, e_attribute_value, y_attribute): \n",
    "    \n",
    "    G_i = dataframe[(dataframe[s_attribute] == s_attribute_value) & (dataframe[e_attribute] == e_attribute_value)].shape[0]\n",
    "    \n",
    "    p_1 = dataframe[(dataframe[y_attribute] == 1) & (dataframe[s_attribute] == s_attribute_value) & (dataframe[e_attribute] == e_attribute_value)].shape[0]/G_i\n",
    "    \n",
    "    p_2_1 = (dataframe[(dataframe[y_attribute] == 1) & (dataframe[s_attribute] == 1) & (dataframe[e_attribute] == e_attribute_value)].shape[0])/(dataframe[(dataframe[s_attribute] == 1) & (dataframe[e_attribute] == e_attribute_value)].shape[0])\n",
    "    p_2_2 = (dataframe[(dataframe[y_attribute] == 1) & (dataframe[s_attribute] == 0) & (dataframe[e_attribute] == e_attribute_value)].shape[0])/(dataframe[(dataframe[s_attribute] == 0) & (dataframe[e_attribute] == e_attribute_value)].shape[0])\n",
    "    p_2 = (p_2_1 + p_2_2) / 2\n",
    "    \n",
    "    return G_i * abs(p_1-p_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to get the accuracy and calibration of each model\n",
    "# Also returns the model in case you want to see confusion matrix\n",
    "# Random train/test not implemented in this script, but the code can be easily adjusted\n",
    "# PARAM is optional, but generally we are only concerned with param=1 because this is what the paper says\n",
    "def evaluate(algorithm, param): \n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # REPLACE HERE WITH FILENAME\n",
    "    # load in the data (already categorized and processed by Ananya)\n",
    "    dataframe = pd.read_csv('../output/cat_encoded_df.csv')\n",
    "\n",
    "    # drop 'vr_charge_degree_column' (missing values)\n",
    "    dataframe = dataframe.drop(columns=['vr_charge_degree'])\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['is_recid'])\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['is_violent_recid'])\n",
    "\n",
    "    train = dataframe[:int(dataframe.shape[0]*0.8)]\n",
    "\n",
    "    test = dataframe[int(dataframe.shape[0]*0.8):]\n",
    "\n",
    "    aa_test = test[test['race'] == 0]\n",
    "    cau_test = test[test['race'] == 1]\n",
    "        \n",
    "    # run local massage algorithm\n",
    "    \n",
    "    if(algorithm==1): \n",
    "        output = local_massage(train, \"race\", \"v_decile_score\", \"two_year_recid\", param)\n",
    "        \n",
    "    if(algorithm==2): \n",
    "        output = local_preferential_sampling(train, \"race\", \"v_decile_score\", \"two_year_recid\", param)\n",
    "\n",
    "    if(algorithm==3): \n",
    "        output = train\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(output.drop(columns=['two_year_recid'], axis=1), output.two_year_recid.copy())\n",
    "    \n",
    "        accuracy = accuracy_score(test.two_year_recid.copy().values, model.predict(test.drop(columns=['two_year_recid'], axis=1)))\n",
    "    \n",
    "        c1 = accuracy_score(aa_test.two_year_recid.copy().values, model.predict(aa_test.drop(columns=['two_year_recid'], axis=1)))\n",
    "    \n",
    "        c2 = accuracy_score(cau_test.two_year_recid.copy().values, model.predict(cau_test.drop(columns=['two_year_recid'], axis=1)))\n",
    "\n",
    "        calibration = abs(c1-c2)\n",
    "    \n",
    "        print(\"Accuracy: \" + str(accuracy))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    # aggregate the output into one final dataframe\n",
    "    output = pd.concat([output[1], output[2], output[3], output[4], output[5], output[6], output[7], output[8], output[9], output[10]])\n",
    "\n",
    "    # drop the probability columns\n",
    "    output = output.drop(columns=['prob_0', 'prob_1', 'dist'])\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(output.drop(['two_year_recid'], axis=1), output.two_year_recid.copy())\n",
    "\n",
    "    accuracy = accuracy_score(test.two_year_recid.copy().values, model.predict(test.drop(['two_year_recid'], axis=1)))\n",
    "    \n",
    "    c1 = accuracy_score(aa_test.two_year_recid.copy().values, model.predict(aa_test.drop(['two_year_recid'], axis=1)))\n",
    "    \n",
    "    c2 = accuracy_score(cau_test.two_year_recid.copy().values, model.predict(cau_test.drop(['two_year_recid'], axis=1)))\n",
    "\n",
    "    calibration = abs(c1-c2)\n",
    "    \n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to get the accuracy and calibration of each model\n",
    "# Also returns the model in case you want to see confusion matrix\n",
    "# Random train/test not implemented in this script, but the code can be easily adjusted\n",
    "# PARAM is optional, but generally we are only concerned with param=1 because this is what the paper says\n",
    "def evaluate(algorithm, param): \n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # REPLACE HERE WITH FILENAME\n",
    "    # load in the data (already categorized and processed by Ananya)\n",
    "    dataframe = pd.read_csv('../output/cat_encoded_df.csv')\n",
    "\n",
    "    # drop 'vr_charge_degree_column' (missing values)\n",
    "    dataframe = dataframe.drop(columns=['vr_charge_degree'])\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['is_recid'])\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['is_violent_recid'])\n",
    "\n",
    "    train = dataframe[:int(dataframe.shape[0]*0.8)]\n",
    "\n",
    "    test = dataframe[int(dataframe.shape[0]*0.8):]\n",
    "\n",
    "    aa_test = test[test['race'] == 0]\n",
    "    cau_test = test[test['race'] == 1]\n",
    "        \n",
    "    # run local massage algorithm\n",
    "    \n",
    "    if(algorithm==1): \n",
    "        output = local_massage(train, \"race\", \"v_decile_score\", \"two_year_recid\", param)\n",
    "        \n",
    "    if(algorithm==2): \n",
    "        output = local_preferential_sampling(train, \"race\", \"v_decile_score\", \"two_year_recid\", param)\n",
    "\n",
    "    if(algorithm==3): \n",
    "        output = train\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(output.drop(columns=['two_year_recid', 'race'], axis=1), output.two_year_recid.copy())\n",
    "    \n",
    "        accuracy = accuracy_score(test.two_year_recid.copy().values, model.predict(test.drop(columns=['two_year_recid', 'race'], axis=1)))\n",
    "    \n",
    "        c1 = accuracy_score(aa_test.two_year_recid.copy().values, model.predict(aa_test.drop(columns=['two_year_recid', 'race'], axis=1)))\n",
    "    \n",
    "        c2 = accuracy_score(cau_test.two_year_recid.copy().values, model.predict(cau_test.drop(columns=['two_year_recid', 'race'], axis=1)))\n",
    "\n",
    "        calibration = abs(c1-c2)\n",
    "    \n",
    "        print(\"Accuracy: \" + str(accuracy))\n",
    "        print(\"Calibration: \" + str(calibration))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    # aggregate the output into one final dataframe\n",
    "    output = pd.concat([output[1], output[2], output[3], output[4], output[5], output[6], output[7], output[8], output[9], output[10]])\n",
    "\n",
    "    # drop the probability columns\n",
    "    output = output.drop(columns=['prob_0', 'prob_1', 'dist'])\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(output.drop(columns = ['two_year_recid' , 'race'], axis=1), output.two_year_recid.copy())\n",
    "\n",
    "    accuracy = accuracy_score(test.two_year_recid.copy().values, model.predict(test.drop(columns = ['two_year_recid' , 'race'], axis=1)))\n",
    "    \n",
    "    c1 = accuracy_score(aa_test.two_year_recid.copy().values, model.predict(aa_test.drop(columns = ['two_year_recid' , 'race'], axis=1)))\n",
    "    \n",
    "    c2 = accuracy_score(cau_test.two_year_recid.copy().values, model.predict(cau_test.drop(columns = ['two_year_recid' , 'race'], axis=1)))\n",
    "\n",
    "    calibration = abs(c1-c2)\n",
    "    \n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.651219512195122\n"
     ]
    }
   ],
   "source": [
    "# Run Local Massaging\n",
    "model = evaluate(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6471544715447154\n"
     ]
    }
   ],
   "source": [
    "# Run Local Preferential Sampling\n",
    "model = evaluate(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6504065040650406\n"
     ]
    }
   ],
   "source": [
    "# Run Baseline\n",
    "model = evaluate(3, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
